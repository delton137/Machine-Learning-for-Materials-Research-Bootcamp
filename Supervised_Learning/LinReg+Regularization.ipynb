{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ridge regression (L2 norm regularization)\n",
    "\n",
    "$min_\\beta ||\\bf{y} - \\bf{X}\\bf{\\beta}||^2_2 + \\gamma||\\bf{\\beta}||^2_2$\n",
    "\n",
    "### Why Ridge Regression is always better\n",
    "\n",
    "#### Stability improvement \n",
    "\n",
    "the distribution of eigenvalues gets regularized, or flattened. This improves the \"stability\" of the matrix when taking the inverse, which is required to solve least squares fitting exactly. \n",
    "\n",
    "#### A Gaussian prior is almost always better than a uniform prior \n",
    "\n",
    "From a Bayesian perspective\n",
    "\n",
    "If we assume a prior $P(\\bf{\\beta, \\eta}) = \\mathcal{N}(0,\\bf{\\Gamma})$ then the Posterior is \n",
    "\n",
    "$\\propto \\sum\\limits_i^n \\left[ \\frac{1}{2\\sigma^2} (\\bf{y}_i - f(x_i))^2 - \\ln \\left(\\frac{1}{\\sigma(2\\pi)^{1/2}} \\right) \\right] + \\bf{\\beta}^T\\bf{\\Gamma}^{-1}\\bf{\\beta}$\n",
    " \n",
    "if the $\\bf{\\beta}$'s are i.i.d. with variance $1/\\gamma$ ie. $\\Gamma = \\frac{1}{\\gamma}\\bf{I}$ then the last term is just $\\gamma ||\\bf{\\beta}||^2$\n",
    "\n",
    "In a standard least squared fit, you are using a prior, without actually using it. It turns out you are using a flat parameter distribution as your prior. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using physics knowledge in construction of the prior\n",
    "\n",
    "\n",
    "\n",
    "How do we set the width of the priors? In the generic ridge regression, the width of the priors to be the same in all cases. However, in many physics problems, you can tune the size of the widths using some physical motivation. For instance, atoms that are closer together will have more effect on the energy of a system as opposed to atoms that are further away. \n",
    "\n",
    "\n",
    "### LASSO regression (L1 norm regularization)\n",
    "\n",
    "LASSO stands for [Least Absolute Shrinkage and Selection Operator](https://en.wikipedia.org/wiki/Lasso_(statistics). \n",
    "\n",
    "Bayesian : [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution) of $P(\\beta_i, \\epsilon) = \\frac{1}{2b} e^{-\\frac{|x-\\mu|}{b}}$\n",
    "\n",
    "In LASSO, the regularization term takes the form\n",
    "$\\sum_j |\\beta_j| \\leq t$\n",
    "\n",
    "\n",
    "Lasso regression is refered to as a \"sparsity inducing\" method, since it tends to supress many weights to zero. In LASSO regression, the \"ball\" of acceptable points in weight space becomes a very pointy object called a simplex. One can show that with probability one, the solution lies one of the points on this simplex. The result is that many of the coefficients may be suppresed to zero. \n",
    "\n",
    "LASSO can be useful for reducing the size of the feature set you are using to the most useful ones. \n",
    "\n",
    "If you rotate your basis when using an L1 norm, then \n",
    "\n",
    "### Elastic net (combination of L1 and L2 regularization) \n",
    "\n",
    "When there is high autocorrelation (?) the LASSO solution is not unique. \n",
    "\n",
    "The Elastic net combines $L_1$ and $L_2$ regularization:\n",
    "$min_\\beta ||\\bf{y} - \\bf{X}\\bf{\\beta}||^2_2 + \\lambda_1||\\bf{\\beta}||_2 \\lambda_2||\\bf{\\beta}||^2_2$\n",
    "\n",
    "\n",
    "In late 2014, it has been proven that the Elastic Net can be reduced to the linear support vector machine ([ref](https://arxiv.org/abs/1303.1152))\n",
    "\n",
    "It can be interpreted in the as a joint Gaussian/Laplacian prior.\n",
    "\n",
    "### Connection to PCA\n",
    "\n",
    "PCA can be reformulated as a least squares optimization with L2 regulariation (ridge regression)\n",
    "\n",
    "### Capturing nonlinearity \n",
    "\n",
    "\n",
    "### Applications of Linear Regression\n",
    "\n",
    "* Local linear regression smoothing \n",
    "* Kernel smoothers \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
